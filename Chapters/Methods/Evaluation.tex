
\section{Evaluation}
\label{sec:evaluation}
An evaluation of the categorization process is essential to know whether the classifier classify correctly or not. This can also be used to find which categories are easy to classify, and which categories are difficult to classify. The evaluation is based on comparing the results with the correct results (called \emph{Gold Standard} \cite[][p. 140]{iirbook}). The gold standard in our project is found in the url of articles, and is decided by the journalists when they publish articles. An article about sport contains \emph{sports} in the url, for example \emph{http://www.rappler.com/sports/by-sport/boxing-mma/pacquiao/90563-mayweather-sr-blasts-ariza}.

\subsection{Evaluation of the Classifier}
Evaluation of the classifier depends on more than just the number of correctly classified categories. A classifier that classify all elements to all classes are not always considered good. There exists numerous ways of evaluating correctness of the classifier. We have chosen \emph{Rand Index accuracy}, \emph{precision}, \emph{recall} and \emph{$F_{1}$-score}. These evaluation measures depend on terms for calculating the results (see table \ref{tab:retrievedescription}). All the measures range from 0 to 1, where 0 is worst and 1 is best.
%Evaluation the categorization is found by evaluating how well the classifier perform, in other words the correctness of the classifier.
%the correctness of the classifier
%The purpose of evaluating the classification is to determine  the correctness of the classifier 
%i.e., how well the classifier perform. 

\subsubsection{RI Accuracy}
\emph{Rand Index} (RI) \emph{accuracy} measures the percentage of decisions that are correctly classified by the classifier \cite[p:~330]{iirbook}. Equation \ref{eq:accuracy} shows how this is computed for evaluating the classifier \cite{ifi:accuracy}. 

\begin{equation} \label{eq:accuracy}
\begin{split}
\text{acc}& = \frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{all documents}\}|}\\
& + \frac{|\{\text{irrelevant documents}\}\cap\{\text{not retrieved documents}\}|}{|\{ \text{all documents}\}|}\\
& = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{FP} + \text{FN} + \text{TN}}
\end{split}
\end{equation}

\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.25}
\begin{tabularx}{\textwidth}{l |X}
\textbf{Term}  & \textbf{Description} \\\hline
\textbf{True Postive} (TP) & Correct: Text is classified to the class by both classifier and \emph{Gold Standard}. \\ \hline
\textbf{True Negative} (TN) &  Correct: Text is neither classified to the class by the classifier, nor by \emph{Gold Standard}.  \\ \hline
\textbf{False Negative} (FN) & Incorrect: Text is not classified to the class by the classifier, but by \emph{Gold Standard}. \\ \hline
\textbf{False Positive} (FP) & Incorrect: Text is classified to the class by the classifier, but not by \emph{Gold Standard}.
\end{tabularx}
\\[10pt]
\caption[Explanation of the terms: \emph{TP}, \emph{TN}, \emph{FN} and \emph{FP}]{Explanation of the \emph{True Positive}, \emph{True Negative}, \emph{False Negative} and \emph{False Positive} \cite[p.~330-331]{iirbook}.}
\label{tab:retrievedescription}
\end{table}

\subsubsection{Precision and Recall}
Another way of evaluating the classifier is by using \emph{precision} and \emph{recall} which measures how many elements are correctly categorized and how many of the correct elements were found. 

%of evaluation categorization is with \emph{precision} and \emph{recall} which are measures of how many elements were correctly categorized \cite{wiki:precisionrecall}. P

Precision is defined as in equation \ref{eq:precision} %\cite{wiki:precisionrecall}
 which measures the fraction of returned results that are relevant \cite[p.~5]{iirbook}. This means that precision can tell how many of the returned articles were categorized to the correct class. 

\begin{equation} \label{eq:precision} 
\begin{split}
\text{precision} & =\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{retrieved documents}\}|} \\
 & = \frac{\text{TP}}{\text{TP} + \text{FP}}
 \end{split}
\end{equation}
Recall is a measure of finding how many of the relevant documents were found by the classifier \cite[p.~5]{iirbook}, which is found in equation \ref{eq:recall}. %\cite{wiki:precisionrecall} 

\begin{equation} \label{eq:recall} 
\begin{split}
\text{recall} & =\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{relevant documents}\}|} \\
 & = \frac{\text{TP}}{\text{TP}+\text{FN}}
\end{split}
\end{equation}

A good classifier should have both high precision and recall. $F_{1}$-score is a combination of precision and recall which gives a measure of the overall evaluation of the classifier based on a the results of precision and recall. The $F_{1}$-score is defined as in equation \ref{eq:fscore}.
%\cite{wiki:fscore}. 

\begin{equation} \label{eq:fscore}
F_1 = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}}
\end{equation}


% TODO: vi kan også evaluaere mappingen mellom wikipedia articler og iab kategorier. 

%\subsection{Evaluate Wikipedia Categorization}
%\subsection{Evaluate Article Categorization}



\begin{comment}
Evaluation is the 


Kan også finne: 
p. 330 i iirbook. 
Rand index : measures the RI percentage of decisions that are correct. 
\end{comment}